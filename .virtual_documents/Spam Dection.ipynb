import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sb 


dataset=pd.read_csv('SPAM text message 20170820 - Data.csv')


dataset.head()


dataset.isnull().sum()


Unique_Category=dataset['Category'].unique()
Unique_Cat={}
i=0
for cat in Unique_Category:
    Unique_Cat[cat]=i
    i+=1
dataset['Category']=dataset['Category'].map(Unique_Cat)


dataset.head(2)


dataset['Message']=dataset['Message'].apply(lambda x:x.lower())





import string
def remove_pun(txt):
    return txt.translate(str.maketrans("","",string.punctuation))


dataset['Message']=dataset['Message'].apply(remove_pun)





def remove_emojis(txt):
    new=""
    for i in txt:
        if i.isascii():
            new=new +i
    return new    
dataset['Message']=dataset['Message'].apply(remove_emojis)





import nltk


from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize


nltk.download("punkt") # Words od libarry
nltk.download("stopwords")


stop_words=set(stopwords.words('english'))


len(stop_words)


def Remove_stopwords(txt):
    words=txt.split()
    Clean_words=[]
    for i in  words:
        if not i in stop_words:
            Clean_words.append(i)
    return ' '.join(Clean_words)  
dataset['Message']=dataset['Message'].apply(Remove_stopwords)   


dataset.loc[1]





from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from imblearn.pipeline import Pipeline 
from sklearn.model_selection import GridSearchCV
from sklearn.feature_extraction.text import TfidfVectorizer
from imblearn.over_sampling import SMOTE
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report


X_train,X_test,y_train,y_test=train_test_split(dataset['Message'] ,dataset['Category'])





pipeline_parameter=[
    ('Tf_vectorizer',TfidfVectorizer()),
    ('smote',SMOTE(random_state=42)),
    ('Mbn',MultinomialNB())
]


pipeline_mbn=Pipeline(pipeline_parameter)


pipeline_mbn.fit(X_train,y_train)


pipeline_mbn.score(X_test,y_test)


pipe_model=pipeline_mbn.predict(X_test)


parameters = {
    'Mbn__alpha': [1.0, 0.5, 0.1] # This name matches your pipeline step
}


grid_search_nb= GridSearchCV(pipeline_mbn, parameters, cv=5, scoring='f1_weighted', n_jobs=1)
grid_search_nb.fit(X_train, y_train)
print("Best alpha:", grid_search_nb.best_params_)


print(classification_report(y_test, grid_search_nb.predict(X_test)))





from sklearn.ensemble import RandomForestClassifier


pipeline_rf = Pipeline([
    ('Tf_vectorizer', TfidfVectorizer(ngram_range=(1, 2), min_df=3, max_df=0.9)),
    ('smote', SMOTE(random_state=42)),
    ('rf', RandomForestClassifier(random_state=42)) 
])


params_rf = {
    'rf__n_estimators': [50, 100, 200], 
    'rf__max_depth': [10, 20, None]
}


grid_search_rf = GridSearchCV(pipeline_rf, params_rf, cv=5, scoring='f1_weighted', n_jobs=1)
grid_search_rf.fit(X_train, y_train)


print(classification_report(y_test, grid_search_rf.predict(X_test)))





pipeline_lr = Pipeline([
    ('Tf_vectorizer', TfidfVectorizer(ngram_range=(1, 2), min_df=3, max_df=0.9)),
    ('smote', SMOTE(random_state=42)),
    # Increase max_iter to ensure the model has enough iterations to converge
    ('lr', LogisticRegression(random_state=42, solver='saga', max_iter=5000))
])


params_lr = {
    'lr__C': [0.1, 1, 10],
    'lr__penalty': ['l1', 'l2']
}


grid_search_lr = GridSearchCV(pipeline_lr, params_lr, cv=5, scoring='f1_weighted', n_jobs=1)
grid_search_lr.fit(X_train, y_train)


print("Best parameters for Logistic Regression:", grid_search_lr.best_params_)
print("\nClassification Report:")
print(classification_report(y_test, grid_search_lr.predict(X_test)))



